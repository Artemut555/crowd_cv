{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0a8535-cdd7-4aae-87fc-72939d8151b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch import amp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from itertools import cycle\n",
    "import gc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc5481a-6eff-47ef-8623-d2cf88c140aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7868ee-2fbd-4108-b086-8398c3bbdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 1. DATA PREPARATION\n",
    "##############################################################################\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (DataFrame): Contains 'downloadUrl' and 'is_conifer'\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "        \"\"\"\n",
    "        self._data = []\n",
    "        for i, row in data.dropna().iterrows():\n",
    "            url = row['downloadUrl']\n",
    "            filename = url.split('/')[-1]\n",
    "            is_conifer = row.get('is_conifer')\n",
    "            self._data.append((filename, is_conifer))\n",
    "        self._root_dir = root_dir\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label = self._data[idx]\n",
    "        img_name = os.path.join(self._root_dir, filename)\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self._transform:\n",
    "            image = self._transform(image)\n",
    "        label = float(label)\n",
    "        return image, label\n",
    "\n",
    "class ImageDatasetWithConfidence(Dataset):\n",
    "    def __init__(self, data, root_dir, transform=None):\n",
    "        self._data = []\n",
    "        for i, row in data.dropna().iterrows():\n",
    "            url = row['downloadUrl']\n",
    "            filename = url.split('/')[-1]\n",
    "            label = row.get('is_conifer')\n",
    "            confidence = row.get('confidence_normalized', 1.0)  # 1.0 if not available\n",
    "            self._data.append((filename, label, confidence))\n",
    "        self._root_dir = root_dir\n",
    "        self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label, confidence = self._data[idx]\n",
    "        img_name = os.path.join(self._root_dir, filename)\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self._transform:\n",
    "            image = self._transform(image)\n",
    "\n",
    "        return image, label, confidence\n",
    "\n",
    "class FixMatchUnlabeledDataset(Dataset):\n",
    "    def __init__(self, data, root_dir, weak_transform=None, strong_transform=None):\n",
    "        self._data = data.reset_index(drop=True)\n",
    "        self._root_dir = root_dir\n",
    "        self.weak_transform = weak_transform\n",
    "        self.strong_transform = strong_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        record = self._data.iloc[idx]\n",
    "        filename = record['downloadUrl'].split('/')[-1]\n",
    "        img_name = os.path.join(self._root_dir, filename)\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.weak_transform:\n",
    "            weak_image = self.weak_transform(image)\n",
    "        else:\n",
    "            weak_image = transforms.ToTensor()(image)\n",
    "\n",
    "        if self.strong_transform:\n",
    "            strong_image = self.strong_transform(image)\n",
    "        else:\n",
    "            strong_image = transforms.ToTensor()(image)\n",
    "\n",
    "        return weak_image, strong_image\n",
    "\n",
    "class FixMatchLabeledDataset(Dataset):\n",
    "    def __init__(self, data, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.records = data.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.records.iloc[idx]\n",
    "        filename = row['downloadUrl'].split('/')[-1]\n",
    "        label = float(row['is_conifer'])\n",
    "        conf = row.get('confidence_normalized', 1.0) ** 2\n",
    "        img_path = os.path.join(self.root_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea13df59-0910-41d1-8eab-0bf9e7eb5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean: tensor([0.2654, 0.3301, 0.2542])\n",
      "Calculated std: tensor([0.1729, 0.1556, 0.1324])\n",
      "The total number of params: 20833873\n",
      "The number of trainable params: 19012297\n",
      "The % of trainable params of total params: 91.26%\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(loader):\n",
    "    \"\"\"Calculate mean and standard deviation for a dataset.\"\"\"\n",
    "    total_sum = torch.zeros(3)\n",
    "    total_squared_sum = torch.zeros(3)\n",
    "    total_count = 0\n",
    "\n",
    "    for images, *_ in loader:\n",
    "        # images: (batch_size, 3, H, W)\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        total_sum += images.sum(dim=(0, 2))\n",
    "        total_squared_sum += (images ** 2).sum(dim=(0, 2))\n",
    "        total_count += images.size(0) * images.size(2)\n",
    "\n",
    "    mean = total_sum / total_count\n",
    "    std = (total_squared_sum / total_count - mean ** 2).sqrt()\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "labeled = pd.read_csv('hw_3_markup_data.txt', sep='\\t')  # 200 images\n",
    "unlabeled = pd.read_csv('hw_3_no_markup_data.txt', sep='\\t', dtype=str)\n",
    "crowdsourced_df = pd.read_csv('train_full.tsv', sep='\\t', skiprows=1,\n",
    "                              names=['downloadUrl', 'is_conifer', 'confidence'], usecols=range(3))\n",
    "crowdsourced_df = crowdsourced_df[~crowdsourced_df['downloadUrl'].isin(labeled['downloadUrl'])]\n",
    "\n",
    "unlabeled = unlabeled[~unlabeled['downloadUrl'].isin(labeled['downloadUrl'])]\n",
    "unlabeled = unlabeled[~unlabeled['downloadUrl'].isin(crowdsourced_df['downloadUrl'])]\n",
    "\n",
    "temp_dataset = ImageDataset(\n",
    "    crowdsourced_df[crowdsourced_df['confidence'] == '100.00%'], # calculate mean without validation\n",
    "    'unlabeled',\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)), \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "temp_loader = DataLoader(temp_dataset, batch_size=64, shuffle=True)\n",
    "train_mean, train_std = calculate_mean_std(temp_loader)\n",
    "del temp_dataset, temp_loader\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Calculated mean:\", train_mean)\n",
    "print(\"Calculated std:\", train_std)\n",
    "\n",
    "##############################################################################\n",
    "# 2. TRANSFORMS FOR FIXMATCH\n",
    "##############################################################################\n",
    "#   - labeled_transform\n",
    "#   - weak_transform\n",
    "#   - strong_transform\n",
    "\n",
    "from torchvision.transforms import RandAugment\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)), #224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "# For Labeled Data\n",
    "labeled_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "# For Unlabeled Data\n",
    "weak_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "strong_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    RandAugment(num_ops=2, magnitude=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std),\n",
    "])\n",
    "\n",
    "##############################################################################\n",
    "# 3. BUILD FIXMATCH DATALOADERS\n",
    "##############################################################################\n",
    "\n",
    "# FILTER CROWD IMAGES BASED ON CONFIDENCE\n",
    "#    - training set: 0.9 <= conf < 1\n",
    "#    - val set: conf = 1\n",
    "#    - unlabeled set: conf < 0.9\n",
    "\n",
    "crowdsourced_df['confidence'] = crowdsourced_df['confidence'].apply(lambda x: float(x[:-1]))\n",
    "max_conf = crowdsourced_df['confidence'].max()\n",
    "crowdsourced_df['confidence_normalized'] = crowdsourced_df['confidence'] / max_conf\n",
    "\n",
    "cs_threshold = 0.9\n",
    "\n",
    "cs_val = crowdsourced_df[crowdsourced_df['confidence_normalized'] == 1.0]\n",
    "cs_train = crowdsourced_df[(crowdsourced_df['confidence_normalized'] >= cs_threshold) &\n",
    "                           (crowdsourced_df['confidence_normalized'] < 1.0)]\n",
    "cs_unlabeled = crowdsourced_df[crowdsourced_df['confidence_normalized'] < cs_threshold]\n",
    "\n",
    "# cs_labeled = crowdsourced_df[crowdsourced_df['confidence_normalized'] >= cs_threshold]\n",
    "# cs_unlabeled = crowdsourced_df[crowdsourced_df['confidence_normalized'] < cs_threshold]\n",
    "\n",
    "combined_labeled_df = pd.concat([cs_train], ignore_index=True)\n",
    "\n",
    "unlabeled_big_df = pd.concat([unlabeled, cs_unlabeled], ignore_index=True)\n",
    "\n",
    "fixmatch_labeled_dataset = FixMatchLabeledDataset(\n",
    "    data=combined_labeled_df,\n",
    "    root_dir='unlabeled',\n",
    "    transform=labeled_transform\n",
    ")\n",
    "\n",
    "fixmatch_unlabeled_dataset = FixMatchUnlabeledDataset(\n",
    "    data=unlabeled_big_df,\n",
    "    root_dir='unlabeled', \n",
    "    weak_transform=weak_transform,\n",
    "    strong_transform=strong_transform\n",
    ")\n",
    "\n",
    "# Build Dataloaders\n",
    "# Typically, we want a ratio of unlabeled to labeled like 4:1 or 5:1 in each batch\n",
    "\n",
    "labeled_batch_size = 48\n",
    "unlabeled_batch_size = 190\n",
    "\n",
    "labeled_loader = DataLoader(fixmatch_labeled_dataset, batch_size=labeled_batch_size, shuffle=True, num_workers=8)\n",
    "unlabeled_loader = DataLoader(fixmatch_unlabeled_dataset, batch_size=unlabeled_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "val_df = pd.concat([labeled, cs_val], ignore_index=True)\n",
    "val_dataset = ImageDataset(val_df, 'labeled', transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "\n",
    "##############################################################################\n",
    "# 4. MODEL DEFINITION\n",
    "##############################################################################\n",
    "model = models.efficientnet_v2_s(weights=\"EfficientNet_V2_S_Weights.IMAGENET1K_V1\")\n",
    "num_in_features_for_class_head = model.classifier[1].in_features\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_in_features_for_class_head, 512), \n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(512, 1)\n",
    ")\n",
    "\n",
    "for features in model.features[:-3]:\n",
    "    for param in features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "num_of_trained_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "num_of_all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"The total number of params: {num_of_all_params}\")\n",
    "print(f\"The number of trainable params: {num_of_trained_params}\")\n",
    "print(f\"The % of trainable params of total params: {(num_of_trained_params / num_of_all_params * 100):.2f}%\")\n",
    "\n",
    "##############################################################################\n",
    "# 5. LOSS, OPTIMIZER, and LR SCHEDULER\n",
    "##############################################################################\n",
    "unsup_criterion = nn.BCEWithLogitsLoss()\n",
    "focal_criterion = FocalLoss(alpha=0.31, gamma=2.0, reduction='mean')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scaler = amp.GradScaler(enabled=(device.type=='cuda'))\n",
    "\n",
    "num_epochs = 100\n",
    "accumulation_steps = 2\n",
    "\n",
    "warmup_epochs = int(0.2 * num_epochs)\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "\n",
    "def warmup_lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return float(epoch + 1) / float(warmup_epochs)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "scheduler_warmup = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
    "\n",
    "##############################################################################\n",
    "# 6. EARLY STOPPING CLASS\n",
    "##############################################################################\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0, verbose=False, path='model_path.pt', mode='max'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            delta (float): Minimum change to qualify as improvement.\n",
    "            verbose (bool): If True, prints a message for each improvement.\n",
    "            path (str): Path to save the best model.\n",
    "            mode (str): 'min' or 'max'.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "        if self.mode == 'min':\n",
    "            self.best_score = np.Inf\n",
    "            self.monitor_op = lambda current, best: current < best - self.delta\n",
    "        elif self.mode == 'max':\n",
    "            self.best_score = -np.Inf\n",
    "            self.monitor_op = lambda current, best: current > best + self.delta\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'min' or 'max'\")\n",
    "\n",
    "    def __call__(self, metric, model):\n",
    "        score = metric\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif self.monitor_op(score, self.best_score):\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        if self.verbose:\n",
    "            print(f'Validation metric improved. Saving model to {self.path}')\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, verbose=True, path='best_fixmatch_model.pt', mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffd7518-d417-4917-a439-d6da01c508e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 7. FIXMATCH TRAINING LOOP\n",
    "##############################################################################\n",
    "threshold = 0.95     # confidence threshold to accept pseudo-label\n",
    "lambda_u = 1.0       # weight for unlabeled loss\n",
    "\n",
    "def train_one_epoch_fixmatch(\n",
    "    epoch, \n",
    "    model, \n",
    "    labeled_loader, \n",
    "    unlabeled_loader, \n",
    "    focal_criterion,\n",
    "    unsup_criterion,\n",
    "    optimizer, \n",
    "    scaler, \n",
    "    accumulation_steps\n",
    "):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # NOTE: size(label) == size(unlab)\n",
    "    # max_batches = max(len(labeled_loader), len(unlabeled_loader)) No enough memory :(\n",
    "    # max_batches = 60\n",
    "    max_batches = min(len(labeled_loader), len(unlabeled_loader))\n",
    "    labeled_iter = iter(cycle(labeled_loader))\n",
    "    unlabeled_iter = iter(cycle(unlabeled_loader))\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        # (x_labeled, y_labeled) = next(labeled_iter)\n",
    "        (x_labeled, y_labeled, conf_labeled) = next(labeled_iter)\n",
    "        \n",
    "\n",
    "        x_labeled = x_labeled.to(device)\n",
    "        y_labeled = y_labeled.to(device)\n",
    "        conf_labeled = conf_labeled.to(device)\n",
    "\n",
    "        (x_weak, x_strong) = next(unlabeled_iter)\n",
    "        x_weak = x_weak.to(device)\n",
    "        x_strong = x_strong.to(device)\n",
    "\n",
    "        # ---------------------------\n",
    "        # 1. SUPERVISED LOSS\n",
    "        # ---------------------------\n",
    "        with amp.autocast('cuda'):\n",
    "            labeled_logits = model(x_labeled).squeeze(1)\n",
    "            raw_loss = focal_criterion(labeled_logits, y_labeled)\n",
    "            sup_loss = (raw_loss * conf_labeled).mean()\n",
    "\n",
    "        # ---------------------------\n",
    "        # 2. UNSUPERVISED LOSS\n",
    "        # ---------------------------\n",
    "        with torch.no_grad():\n",
    "            weak_logits = model(x_weak).squeeze(1)\n",
    "            weak_probs = torch.sigmoid(weak_logits)\n",
    "            pseudo_label = (weak_probs > 0.5).float()\n",
    "            mask = (weak_probs > threshold).float()\n",
    "\n",
    "        with amp.autocast('cuda'):\n",
    "            strong_logits = model(x_strong).squeeze(1)\n",
    "            unsup_loss = unsup_criterion(strong_logits, pseudo_label)\n",
    "            unsup_loss = (unsup_loss * mask).mean()\n",
    "\n",
    "            total_loss = sup_loss + lambda_u * unsup_loss\n",
    "\n",
    "        scaler.scale(total_loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"[Epoch {epoch+1}, Batch {batch_idx+1}] SupLoss: {sup_loss.item():.4f}, UnsupLoss: {unsup_loss.item():.4f}, TotalLoss: {total_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    if max_batches % accumulation_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # scheduler.step()\n",
    "    avg_loss = running_loss / max_batches\n",
    "    print(f\">>> [Epoch {epoch+1}] FixMatch Train Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "\n",
    "def validate_fixmatch(model, val_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            logits = model(x_val).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            predicted = (probs > 0.5).float()\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "            all_labels.extend(y_val.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, pos_label=1)\n",
    "    print(f\"Validation Accuracy: {acc:.2f}%, F1-Score: {f1:.4f}\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b17b529-d5ac-4daa-a431-9ac8b147746b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632836effab3441482d50323bfe1d675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch 10] SupLoss: 0.0511, UnsupLoss: 0.0000, TotalLoss: 0.0511\n",
      "[Epoch 1, Batch 20] SupLoss: 0.0506, UnsupLoss: 0.0000, TotalLoss: 0.0506\n",
      ">>> [Epoch 1] FixMatch Train Loss: 0.0503, LR: 0.000005\n",
      "Validation Accuracy: 75.47%, F1-Score: 0.1333\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 2, Batch 10] SupLoss: 0.0506, UnsupLoss: 0.0000, TotalLoss: 0.0506\n",
      "[Epoch 2, Batch 20] SupLoss: 0.0478, UnsupLoss: 0.0000, TotalLoss: 0.0478\n",
      ">>> [Epoch 2] FixMatch Train Loss: 0.0486, LR: 0.000010\n",
      "Validation Accuracy: 77.36%, F1-Score: 0.1429\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 3, Batch 10] SupLoss: 0.0434, UnsupLoss: 0.0000, TotalLoss: 0.0434\n",
      "[Epoch 3, Batch 20] SupLoss: 0.0433, UnsupLoss: 0.0000, TotalLoss: 0.0433\n",
      ">>> [Epoch 3] FixMatch Train Loss: 0.0469, LR: 0.000015\n",
      "Validation Accuracy: 77.36%, F1-Score: 0.1429\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 4, Batch 10] SupLoss: 0.0461, UnsupLoss: 0.0000, TotalLoss: 0.0461\n",
      "[Epoch 4, Batch 20] SupLoss: 0.0447, UnsupLoss: 0.0000, TotalLoss: 0.0447\n",
      ">>> [Epoch 4] FixMatch Train Loss: 0.0451, LR: 0.000020\n",
      "Validation Accuracy: 79.25%, F1-Score: 0.1538\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 5, Batch 10] SupLoss: 0.0472, UnsupLoss: 0.0000, TotalLoss: 0.0472\n",
      "[Epoch 5, Batch 20] SupLoss: 0.0452, UnsupLoss: 0.0000, TotalLoss: 0.0452\n",
      ">>> [Epoch 5] FixMatch Train Loss: 0.0428, LR: 0.000025\n",
      "Validation Accuracy: 81.13%, F1-Score: 0.2857\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 6, Batch 10] SupLoss: 0.0418, UnsupLoss: 0.0000, TotalLoss: 0.0418\n",
      "[Epoch 6, Batch 20] SupLoss: 0.0335, UnsupLoss: 0.0000, TotalLoss: 0.0335\n",
      ">>> [Epoch 6] FixMatch Train Loss: 0.0399, LR: 0.000030\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.6667\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 7, Batch 10] SupLoss: 0.0363, UnsupLoss: 0.0000, TotalLoss: 0.0363\n",
      "[Epoch 7, Batch 20] SupLoss: 0.0344, UnsupLoss: 0.0000, TotalLoss: 0.0344\n",
      ">>> [Epoch 7] FixMatch Train Loss: 0.0366, LR: 0.000035\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6316\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 8, Batch 10] SupLoss: 0.0276, UnsupLoss: 0.0000, TotalLoss: 0.0276\n",
      "[Epoch 8, Batch 20] SupLoss: 0.0317, UnsupLoss: 0.0000, TotalLoss: 0.0317\n",
      ">>> [Epoch 8] FixMatch Train Loss: 0.0326, LR: 0.000040\n",
      "Validation Accuracy: 83.02%, F1-Score: 0.6087\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Epoch 9, Batch 10] SupLoss: 0.0291, UnsupLoss: 0.0000, TotalLoss: 0.0291\n",
      "[Epoch 9, Batch 20] SupLoss: 0.0247, UnsupLoss: 0.0000, TotalLoss: 0.0247\n",
      ">>> [Epoch 9] FixMatch Train Loss: 0.0289, LR: 0.000045\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 10, Batch 10] SupLoss: 0.0166, UnsupLoss: 0.0000, TotalLoss: 0.0166\n",
      "[Epoch 10, Batch 20] SupLoss: 0.0152, UnsupLoss: 0.0000, TotalLoss: 0.0152\n",
      ">>> [Epoch 10] FixMatch Train Loss: 0.0261, LR: 0.000050\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 11, Batch 10] SupLoss: 0.0222, UnsupLoss: 0.0000, TotalLoss: 0.0222\n",
      "[Epoch 11, Batch 20] SupLoss: 0.0213, UnsupLoss: 0.0000, TotalLoss: 0.0213\n",
      ">>> [Epoch 11] FixMatch Train Loss: 0.0241, LR: 0.000055\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Epoch 12, Batch 10] SupLoss: 0.0175, UnsupLoss: 0.0000, TotalLoss: 0.0175\n",
      "[Epoch 12, Batch 20] SupLoss: 0.0274, UnsupLoss: 0.0000, TotalLoss: 0.0274\n",
      ">>> [Epoch 12] FixMatch Train Loss: 0.0210, LR: 0.000060\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8571\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 13, Batch 10] SupLoss: 0.0212, UnsupLoss: 0.0000, TotalLoss: 0.0212\n",
      "[Epoch 13, Batch 20] SupLoss: 0.0167, UnsupLoss: 0.0000, TotalLoss: 0.0167\n",
      ">>> [Epoch 13] FixMatch Train Loss: 0.0185, LR: 0.000065\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 14, Batch 10] SupLoss: 0.0174, UnsupLoss: 0.0066, TotalLoss: 0.0239\n",
      "[Epoch 14, Batch 20] SupLoss: 0.0152, UnsupLoss: 0.0046, TotalLoss: 0.0198\n",
      ">>> [Epoch 14] FixMatch Train Loss: 0.0172, LR: 0.000070\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Epoch 15, Batch 10] SupLoss: 0.0168, UnsupLoss: 0.0041, TotalLoss: 0.0209\n",
      "[Epoch 15, Batch 20] SupLoss: 0.0160, UnsupLoss: 0.0086, TotalLoss: 0.0246\n",
      ">>> [Epoch 15] FixMatch Train Loss: 0.0216, LR: 0.000075\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Epoch 16, Batch 10] SupLoss: 0.0141, UnsupLoss: 0.0141, TotalLoss: 0.0282\n",
      "[Epoch 16, Batch 20] SupLoss: 0.0044, UnsupLoss: 0.0090, TotalLoss: 0.0134\n",
      ">>> [Epoch 16] FixMatch Train Loss: 0.0258, LR: 0.000080\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7000\n",
      "EarlyStopping counter: 4 out of 50\n",
      "[Epoch 17, Batch 10] SupLoss: 0.0145, UnsupLoss: 0.0219, TotalLoss: 0.0363\n",
      "[Epoch 17, Batch 20] SupLoss: 0.0082, UnsupLoss: 0.0168, TotalLoss: 0.0250\n",
      ">>> [Epoch 17] FixMatch Train Loss: 0.0246, LR: 0.000085\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 5 out of 50\n",
      "[Epoch 18, Batch 10] SupLoss: 0.0086, UnsupLoss: 0.0161, TotalLoss: 0.0247\n",
      "[Epoch 18, Batch 20] SupLoss: 0.0043, UnsupLoss: 0.0157, TotalLoss: 0.0200\n",
      ">>> [Epoch 18] FixMatch Train Loss: 0.0235, LR: 0.000090\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.7200\n",
      "EarlyStopping counter: 6 out of 50\n",
      "[Epoch 19, Batch 10] SupLoss: 0.0103, UnsupLoss: 0.0152, TotalLoss: 0.0255\n",
      "[Epoch 19, Batch 20] SupLoss: 0.0070, UnsupLoss: 0.0158, TotalLoss: 0.0228\n",
      ">>> [Epoch 19] FixMatch Train Loss: 0.0214, LR: 0.000095\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7619\n",
      "EarlyStopping counter: 7 out of 50\n",
      "[Epoch 20, Batch 10] SupLoss: 0.0080, UnsupLoss: 0.0160, TotalLoss: 0.0240\n",
      "[Epoch 20, Batch 20] SupLoss: 0.0055, UnsupLoss: 0.0216, TotalLoss: 0.0271\n",
      ">>> [Epoch 20] FixMatch Train Loss: 0.0292, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 8 out of 50\n",
      "[Epoch 21, Batch 10] SupLoss: 0.0078, UnsupLoss: 0.0269, TotalLoss: 0.0347\n",
      "[Epoch 21, Batch 20] SupLoss: 0.0024, UnsupLoss: 0.0187, TotalLoss: 0.0211\n",
      ">>> [Epoch 21] FixMatch Train Loss: 0.0268, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 9 out of 50\n",
      "[Epoch 22, Batch 10] SupLoss: 0.0096, UnsupLoss: 0.0244, TotalLoss: 0.0339\n",
      "[Epoch 22, Batch 20] SupLoss: 0.0021, UnsupLoss: 0.0171, TotalLoss: 0.0192\n",
      ">>> [Epoch 22] FixMatch Train Loss: 0.0278, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 10 out of 50\n",
      "[Epoch 23, Batch 10] SupLoss: 0.0047, UnsupLoss: 0.0203, TotalLoss: 0.0251\n",
      "[Epoch 23, Batch 20] SupLoss: 0.0084, UnsupLoss: 0.0190, TotalLoss: 0.0274\n",
      ">>> [Epoch 23] FixMatch Train Loss: 0.0248, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 11 out of 50\n",
      "[Epoch 24, Batch 10] SupLoss: 0.0036, UnsupLoss: 0.0186, TotalLoss: 0.0222\n",
      "[Epoch 24, Batch 20] SupLoss: 0.0043, UnsupLoss: 0.0159, TotalLoss: 0.0202\n",
      ">>> [Epoch 24] FixMatch Train Loss: 0.0233, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 12 out of 50\n",
      "[Epoch 25, Batch 10] SupLoss: 0.0017, UnsupLoss: 0.0286, TotalLoss: 0.0303\n",
      "[Epoch 25, Batch 20] SupLoss: 0.0039, UnsupLoss: 0.0114, TotalLoss: 0.0153\n",
      ">>> [Epoch 25] FixMatch Train Loss: 0.0234, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 13 out of 50\n",
      "[Epoch 26, Batch 10] SupLoss: 0.0023, UnsupLoss: 0.0189, TotalLoss: 0.0211\n",
      "[Epoch 26, Batch 20] SupLoss: 0.0037, UnsupLoss: 0.0223, TotalLoss: 0.0261\n",
      ">>> [Epoch 26] FixMatch Train Loss: 0.0251, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.7200\n",
      "EarlyStopping counter: 14 out of 50\n",
      "[Epoch 27, Batch 10] SupLoss: 0.0010, UnsupLoss: 0.0280, TotalLoss: 0.0290\n",
      "[Epoch 27, Batch 20] SupLoss: 0.0044, UnsupLoss: 0.0179, TotalLoss: 0.0224\n",
      ">>> [Epoch 27] FixMatch Train Loss: 0.0263, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7619\n",
      "EarlyStopping counter: 15 out of 50\n",
      "[Epoch 28, Batch 10] SupLoss: 0.0105, UnsupLoss: 0.0216, TotalLoss: 0.0321\n",
      "[Epoch 28, Batch 20] SupLoss: 0.0015, UnsupLoss: 0.0154, TotalLoss: 0.0169\n",
      ">>> [Epoch 28] FixMatch Train Loss: 0.0247, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 16 out of 50\n",
      "[Epoch 29, Batch 10] SupLoss: 0.0013, UnsupLoss: 0.0192, TotalLoss: 0.0205\n",
      "[Epoch 29, Batch 20] SupLoss: 0.0008, UnsupLoss: 0.0360, TotalLoss: 0.0368\n",
      ">>> [Epoch 29] FixMatch Train Loss: 0.0298, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7619\n",
      "EarlyStopping counter: 17 out of 50\n",
      "[Epoch 30, Batch 10] SupLoss: 0.0014, UnsupLoss: 0.0236, TotalLoss: 0.0250\n",
      "[Epoch 30, Batch 20] SupLoss: 0.0042, UnsupLoss: 0.0187, TotalLoss: 0.0229\n",
      ">>> [Epoch 30] FixMatch Train Loss: 0.0267, LR: 0.000100\n",
      "Validation Accuracy: 84.91%, F1-Score: 0.6364\n",
      "EarlyStopping counter: 18 out of 50\n",
      "[Epoch 31, Batch 10] SupLoss: 0.0041, UnsupLoss: 0.0100, TotalLoss: 0.0141\n",
      "[Epoch 31, Batch 20] SupLoss: 0.0131, UnsupLoss: 0.0151, TotalLoss: 0.0282\n",
      ">>> [Epoch 31] FixMatch Train Loss: 0.0233, LR: 0.000100\n",
      "Validation Accuracy: 84.91%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 19 out of 50\n",
      "[Epoch 32, Batch 10] SupLoss: 0.0015, UnsupLoss: 0.0375, TotalLoss: 0.0390\n",
      "[Epoch 32, Batch 20] SupLoss: 0.0008, UnsupLoss: 0.0124, TotalLoss: 0.0131\n",
      ">>> [Epoch 32] FixMatch Train Loss: 0.0268, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 20 out of 50\n",
      "[Epoch 33, Batch 10] SupLoss: 0.0011, UnsupLoss: 0.0196, TotalLoss: 0.0207\n",
      "[Epoch 33, Batch 20] SupLoss: 0.0041, UnsupLoss: 0.0270, TotalLoss: 0.0311\n",
      ">>> [Epoch 33] FixMatch Train Loss: 0.0248, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 21 out of 50\n",
      "[Epoch 34, Batch 10] SupLoss: 0.0040, UnsupLoss: 0.0224, TotalLoss: 0.0264\n",
      "[Epoch 34, Batch 20] SupLoss: 0.0019, UnsupLoss: 0.0298, TotalLoss: 0.0316\n",
      ">>> [Epoch 34] FixMatch Train Loss: 0.0277, LR: 0.000100\n",
      "Validation Accuracy: 84.91%, F1-Score: 0.6364\n",
      "EarlyStopping counter: 22 out of 50\n",
      "[Epoch 35, Batch 10] SupLoss: 0.0004, UnsupLoss: 0.0199, TotalLoss: 0.0203\n",
      "[Epoch 35, Batch 20] SupLoss: 0.0048, UnsupLoss: 0.0182, TotalLoss: 0.0231\n",
      ">>> [Epoch 35] FixMatch Train Loss: 0.0254, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 23 out of 50\n",
      "[Epoch 36, Batch 10] SupLoss: 0.0014, UnsupLoss: 0.0163, TotalLoss: 0.0177\n",
      "[Epoch 36, Batch 20] SupLoss: 0.0010, UnsupLoss: 0.0188, TotalLoss: 0.0198\n",
      ">>> [Epoch 36] FixMatch Train Loss: 0.0227, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7826\n",
      "EarlyStopping counter: 24 out of 50\n",
      "[Epoch 37, Batch 10] SupLoss: 0.0043, UnsupLoss: 0.0377, TotalLoss: 0.0419\n",
      "[Epoch 37, Batch 20] SupLoss: 0.0030, UnsupLoss: 0.0390, TotalLoss: 0.0420\n",
      ">>> [Epoch 37] FixMatch Train Loss: 0.0342, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 25 out of 50\n",
      "[Epoch 38, Batch 10] SupLoss: 0.0013, UnsupLoss: 0.0295, TotalLoss: 0.0308\n",
      "[Epoch 38, Batch 20] SupLoss: 0.0055, UnsupLoss: 0.0266, TotalLoss: 0.0321\n",
      ">>> [Epoch 38] FixMatch Train Loss: 0.0308, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 26 out of 50\n",
      "[Epoch 39, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0260, TotalLoss: 0.0261\n",
      "[Epoch 39, Batch 20] SupLoss: 0.0049, UnsupLoss: 0.0303, TotalLoss: 0.0352\n",
      ">>> [Epoch 39] FixMatch Train Loss: 0.0307, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 27 out of 50\n",
      "[Epoch 40, Batch 10] SupLoss: 0.0040, UnsupLoss: 0.0225, TotalLoss: 0.0265\n",
      "[Epoch 40, Batch 20] SupLoss: 0.0034, UnsupLoss: 0.0352, TotalLoss: 0.0386\n",
      ">>> [Epoch 40] FixMatch Train Loss: 0.0306, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 28 out of 50\n",
      "[Epoch 41, Batch 10] SupLoss: 0.0055, UnsupLoss: 0.0199, TotalLoss: 0.0255\n",
      "[Epoch 41, Batch 20] SupLoss: 0.0006, UnsupLoss: 0.0263, TotalLoss: 0.0269\n",
      ">>> [Epoch 41] FixMatch Train Loss: 0.0292, LR: 0.000100\n",
      "Validation Accuracy: 84.91%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 29 out of 50\n",
      "[Epoch 42, Batch 10] SupLoss: 0.0006, UnsupLoss: 0.0315, TotalLoss: 0.0321\n",
      "[Epoch 42, Batch 20] SupLoss: 0.0020, UnsupLoss: 0.0258, TotalLoss: 0.0278\n",
      ">>> [Epoch 42] FixMatch Train Loss: 0.0282, LR: 0.000100\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8696\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 43, Batch 10] SupLoss: 0.0022, UnsupLoss: 0.0279, TotalLoss: 0.0301\n",
      "[Epoch 43, Batch 20] SupLoss: 0.0058, UnsupLoss: 0.0255, TotalLoss: 0.0313\n",
      ">>> [Epoch 43] FixMatch Train Loss: 0.0289, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 44, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0320, TotalLoss: 0.0325\n",
      "[Epoch 44, Batch 20] SupLoss: 0.0006, UnsupLoss: 0.0256, TotalLoss: 0.0262\n",
      ">>> [Epoch 44] FixMatch Train Loss: 0.0321, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.7200\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Epoch 45, Batch 10] SupLoss: 0.0010, UnsupLoss: 0.0213, TotalLoss: 0.0223\n",
      "[Epoch 45, Batch 20] SupLoss: 0.0014, UnsupLoss: 0.0292, TotalLoss: 0.0306\n",
      ">>> [Epoch 45] FixMatch Train Loss: 0.0301, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Epoch 46, Batch 10] SupLoss: 0.0008, UnsupLoss: 0.0258, TotalLoss: 0.0266\n",
      "[Epoch 46, Batch 20] SupLoss: 0.0048, UnsupLoss: 0.0204, TotalLoss: 0.0253\n",
      ">>> [Epoch 46] FixMatch Train Loss: 0.0265, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7826\n",
      "EarlyStopping counter: 4 out of 50\n",
      "[Epoch 47, Batch 10] SupLoss: 0.0036, UnsupLoss: 0.0430, TotalLoss: 0.0466\n",
      "[Epoch 47, Batch 20] SupLoss: 0.0059, UnsupLoss: 0.0208, TotalLoss: 0.0267\n",
      ">>> [Epoch 47] FixMatch Train Loss: 0.0285, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6667\n",
      "EarlyStopping counter: 5 out of 50\n",
      "[Epoch 48, Batch 10] SupLoss: 0.0016, UnsupLoss: 0.0310, TotalLoss: 0.0326\n",
      "[Epoch 48, Batch 20] SupLoss: 0.0070, UnsupLoss: 0.0240, TotalLoss: 0.0310\n",
      ">>> [Epoch 48] FixMatch Train Loss: 0.0275, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 6 out of 50\n",
      "[Epoch 49, Batch 10] SupLoss: 0.0003, UnsupLoss: 0.0423, TotalLoss: 0.0426\n",
      "[Epoch 49, Batch 20] SupLoss: 0.0010, UnsupLoss: 0.0391, TotalLoss: 0.0401\n",
      ">>> [Epoch 49] FixMatch Train Loss: 0.0365, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7000\n",
      "EarlyStopping counter: 7 out of 50\n",
      "[Epoch 50, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0352, TotalLoss: 0.0357\n",
      "[Epoch 50, Batch 20] SupLoss: 0.0008, UnsupLoss: 0.0298, TotalLoss: 0.0306\n",
      ">>> [Epoch 50] FixMatch Train Loss: 0.0374, LR: 0.000100\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 8 out of 50\n",
      "[Epoch 51, Batch 10] SupLoss: 0.0006, UnsupLoss: 0.0439, TotalLoss: 0.0445\n",
      "[Epoch 51, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0290, TotalLoss: 0.0292\n",
      ">>> [Epoch 51] FixMatch Train Loss: 0.0320, LR: 0.000100\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 9 out of 50\n",
      "[Epoch 52, Batch 10] SupLoss: 0.0020, UnsupLoss: 0.0238, TotalLoss: 0.0258\n",
      "[Epoch 52, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0259, TotalLoss: 0.0261\n",
      ">>> [Epoch 52] FixMatch Train Loss: 0.0326, LR: 0.000100\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8182\n",
      "EarlyStopping counter: 10 out of 50\n",
      "[Epoch 53, Batch 10] SupLoss: 0.0040, UnsupLoss: 0.0235, TotalLoss: 0.0275\n",
      "[Epoch 53, Batch 20] SupLoss: 0.0011, UnsupLoss: 0.0339, TotalLoss: 0.0350\n",
      ">>> [Epoch 53] FixMatch Train Loss: 0.0307, LR: 0.000100\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7619\n",
      "EarlyStopping counter: 11 out of 50\n",
      "[Epoch 54, Batch 10] SupLoss: 0.0035, UnsupLoss: 0.0303, TotalLoss: 0.0338\n",
      "[Epoch 54, Batch 20] SupLoss: 0.0022, UnsupLoss: 0.0263, TotalLoss: 0.0285\n",
      ">>> [Epoch 54] FixMatch Train Loss: 0.0349, LR: 0.000050\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.7826\n",
      "EarlyStopping counter: 12 out of 50\n",
      "[Epoch 55, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0315, TotalLoss: 0.0316\n",
      "[Epoch 55, Batch 20] SupLoss: 0.0003, UnsupLoss: 0.0328, TotalLoss: 0.0330\n",
      ">>> [Epoch 55] FixMatch Train Loss: 0.0329, LR: 0.000050\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8182\n",
      "EarlyStopping counter: 13 out of 50\n",
      "[Epoch 56, Batch 10] SupLoss: 0.0011, UnsupLoss: 0.0323, TotalLoss: 0.0334\n",
      "[Epoch 56, Batch 20] SupLoss: 0.0023, UnsupLoss: 0.0327, TotalLoss: 0.0350\n",
      ">>> [Epoch 56] FixMatch Train Loss: 0.0319, LR: 0.000050\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8696\n",
      "EarlyStopping counter: 14 out of 50\n",
      "[Epoch 57, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0270, TotalLoss: 0.0276\n",
      "[Epoch 57, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0308, TotalLoss: 0.0309\n",
      ">>> [Epoch 57] FixMatch Train Loss: 0.0298, LR: 0.000050\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8182\n",
      "EarlyStopping counter: 15 out of 50\n",
      "[Epoch 58, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0276, TotalLoss: 0.0277\n",
      "[Epoch 58, Batch 20] SupLoss: 0.0004, UnsupLoss: 0.0283, TotalLoss: 0.0286\n",
      ">>> [Epoch 58] FixMatch Train Loss: 0.0315, LR: 0.000050\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8148\n",
      "EarlyStopping counter: 16 out of 50\n",
      "[Epoch 59, Batch 10] SupLoss: 0.0004, UnsupLoss: 0.0298, TotalLoss: 0.0302\n",
      "[Epoch 59, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0281, TotalLoss: 0.0282\n",
      ">>> [Epoch 59] FixMatch Train Loss: 0.0319, LR: 0.000050\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 17 out of 50\n",
      "[Epoch 60, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0237, TotalLoss: 0.0239\n",
      "[Epoch 60, Batch 20] SupLoss: 0.0028, UnsupLoss: 0.0179, TotalLoss: 0.0206\n",
      ">>> [Epoch 60] FixMatch Train Loss: 0.0270, LR: 0.000050\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7273\n",
      "EarlyStopping counter: 18 out of 50\n",
      "[Epoch 61, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0262, TotalLoss: 0.0263\n",
      "[Epoch 61, Batch 20] SupLoss: 0.0005, UnsupLoss: 0.0200, TotalLoss: 0.0206\n",
      ">>> [Epoch 61] FixMatch Train Loss: 0.0340, LR: 0.000050\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7692\n",
      "EarlyStopping counter: 19 out of 50\n",
      "[Epoch 62, Batch 10] SupLoss: 0.0004, UnsupLoss: 0.0342, TotalLoss: 0.0345\n",
      "[Epoch 62, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0257, TotalLoss: 0.0259\n",
      ">>> [Epoch 62] FixMatch Train Loss: 0.0314, LR: 0.000050\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 20 out of 50\n",
      "[Epoch 63, Batch 10] SupLoss: 0.0021, UnsupLoss: 0.0189, TotalLoss: 0.0210\n",
      "[Epoch 63, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0195, TotalLoss: 0.0198\n",
      ">>> [Epoch 63] FixMatch Train Loss: 0.0285, LR: 0.000050\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 21 out of 50\n",
      "[Epoch 64, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0246, TotalLoss: 0.0248\n",
      "[Epoch 64, Batch 20] SupLoss: 0.0015, UnsupLoss: 0.0228, TotalLoss: 0.0244\n",
      ">>> [Epoch 64] FixMatch Train Loss: 0.0276, LR: 0.000050\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 22 out of 50\n",
      "[Epoch 65, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0272, TotalLoss: 0.0277\n",
      "[Epoch 65, Batch 20] SupLoss: 0.0034, UnsupLoss: 0.0304, TotalLoss: 0.0337\n",
      ">>> [Epoch 65] FixMatch Train Loss: 0.0274, LR: 0.000025\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7692\n",
      "EarlyStopping counter: 23 out of 50\n",
      "[Epoch 66, Batch 10] SupLoss: 0.0016, UnsupLoss: 0.0339, TotalLoss: 0.0356\n",
      "[Epoch 66, Batch 20] SupLoss: 0.0013, UnsupLoss: 0.0292, TotalLoss: 0.0305\n",
      ">>> [Epoch 66] FixMatch Train Loss: 0.0301, LR: 0.000025\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 24 out of 50\n",
      "[Epoch 67, Batch 10] SupLoss: 0.0004, UnsupLoss: 0.0221, TotalLoss: 0.0226\n",
      "[Epoch 67, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0296, TotalLoss: 0.0297\n",
      ">>> [Epoch 67] FixMatch Train Loss: 0.0285, LR: 0.000025\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7857\n",
      "EarlyStopping counter: 25 out of 50\n",
      "[Epoch 68, Batch 10] SupLoss: 0.0003, UnsupLoss: 0.0278, TotalLoss: 0.0282\n",
      "[Epoch 68, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0248, TotalLoss: 0.0249\n",
      ">>> [Epoch 68] FixMatch Train Loss: 0.0266, LR: 0.000025\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 26 out of 50\n",
      "[Epoch 69, Batch 10] SupLoss: 0.0000, UnsupLoss: 0.0339, TotalLoss: 0.0339\n",
      "[Epoch 69, Batch 20] SupLoss: 0.0003, UnsupLoss: 0.0201, TotalLoss: 0.0205\n",
      ">>> [Epoch 69] FixMatch Train Loss: 0.0284, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 27 out of 50\n",
      "[Epoch 70, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0271, TotalLoss: 0.0272\n",
      "[Epoch 70, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0257, TotalLoss: 0.0258\n",
      ">>> [Epoch 70] FixMatch Train Loss: 0.0284, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 28 out of 50\n",
      "[Epoch 71, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0288, TotalLoss: 0.0289\n",
      "[Epoch 71, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0280, TotalLoss: 0.0280\n",
      ">>> [Epoch 71] FixMatch Train Loss: 0.0274, LR: 0.000025\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 29 out of 50\n",
      "[Epoch 72, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0379, TotalLoss: 0.0380\n",
      "[Epoch 72, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0281, TotalLoss: 0.0281\n",
      ">>> [Epoch 72] FixMatch Train Loss: 0.0310, LR: 0.000025\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "Validation metric improved. Saving model to best_fixmatch_model.pt\n",
      "[Epoch 73, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0271, TotalLoss: 0.0276\n",
      "[Epoch 73, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0301, TotalLoss: 0.0302\n",
      ">>> [Epoch 73] FixMatch Train Loss: 0.0294, LR: 0.000025\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 1 out of 50\n",
      "[Epoch 74, Batch 10] SupLoss: 0.0052, UnsupLoss: 0.0315, TotalLoss: 0.0366\n",
      "[Epoch 74, Batch 20] SupLoss: 0.0004, UnsupLoss: 0.0353, TotalLoss: 0.0357\n",
      ">>> [Epoch 74] FixMatch Train Loss: 0.0332, LR: 0.000025\n",
      "Validation Accuracy: 84.91%, F1-Score: 0.7143\n",
      "EarlyStopping counter: 2 out of 50\n",
      "[Epoch 75, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0279, TotalLoss: 0.0281\n",
      "[Epoch 75, Batch 20] SupLoss: 0.0011, UnsupLoss: 0.0318, TotalLoss: 0.0329\n",
      ">>> [Epoch 75] FixMatch Train Loss: 0.0308, LR: 0.000025\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7692\n",
      "EarlyStopping counter: 3 out of 50\n",
      "[Epoch 76, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0313, TotalLoss: 0.0315\n",
      "[Epoch 76, Batch 20] SupLoss: 0.0027, UnsupLoss: 0.0378, TotalLoss: 0.0405\n",
      ">>> [Epoch 76] FixMatch Train Loss: 0.0328, LR: 0.000025\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.7586\n",
      "EarlyStopping counter: 4 out of 50\n",
      "[Epoch 77, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0332, TotalLoss: 0.0337\n",
      "[Epoch 77, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0305, TotalLoss: 0.0306\n",
      ">>> [Epoch 77] FixMatch Train Loss: 0.0332, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 5 out of 50\n",
      "[Epoch 78, Batch 10] SupLoss: 0.0013, UnsupLoss: 0.0367, TotalLoss: 0.0380\n",
      "[Epoch 78, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0388, TotalLoss: 0.0389\n",
      ">>> [Epoch 78] FixMatch Train Loss: 0.0330, LR: 0.000025\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "EarlyStopping counter: 6 out of 50\n",
      "[Epoch 79, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0363, TotalLoss: 0.0364\n",
      "[Epoch 79, Batch 20] SupLoss: 0.0003, UnsupLoss: 0.0308, TotalLoss: 0.0312\n",
      ">>> [Epoch 79] FixMatch Train Loss: 0.0303, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 7 out of 50\n",
      "[Epoch 80, Batch 10] SupLoss: 0.0009, UnsupLoss: 0.0278, TotalLoss: 0.0288\n",
      "[Epoch 80, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0328, TotalLoss: 0.0330\n",
      ">>> [Epoch 80] FixMatch Train Loss: 0.0308, LR: 0.000025\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 8 out of 50\n",
      "[Epoch 81, Batch 10] SupLoss: 0.0000, UnsupLoss: 0.0321, TotalLoss: 0.0321\n",
      "[Epoch 81, Batch 20] SupLoss: 0.0006, UnsupLoss: 0.0249, TotalLoss: 0.0255\n",
      ">>> [Epoch 81] FixMatch Train Loss: 0.0318, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 9 out of 50\n",
      "[Epoch 82, Batch 10] SupLoss: 0.0006, UnsupLoss: 0.0325, TotalLoss: 0.0331\n",
      "[Epoch 82, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0274, TotalLoss: 0.0274\n",
      ">>> [Epoch 82] FixMatch Train Loss: 0.0319, LR: 0.000025\n",
      "Validation Accuracy: 86.79%, F1-Score: 0.6957\n",
      "EarlyStopping counter: 10 out of 50\n",
      "[Epoch 83, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0271, TotalLoss: 0.0272\n",
      "[Epoch 83, Batch 20] SupLoss: 0.0006, UnsupLoss: 0.0369, TotalLoss: 0.0375\n",
      ">>> [Epoch 83] FixMatch Train Loss: 0.0301, LR: 0.000025\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 11 out of 50\n",
      "[Epoch 84, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0217, TotalLoss: 0.0219\n",
      "[Epoch 84, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0260, TotalLoss: 0.0261\n",
      ">>> [Epoch 84] FixMatch Train Loss: 0.0308, LR: 0.000013\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 12 out of 50\n",
      "[Epoch 85, Batch 10] SupLoss: 0.0000, UnsupLoss: 0.0259, TotalLoss: 0.0259\n",
      "[Epoch 85, Batch 20] SupLoss: 0.0010, UnsupLoss: 0.0309, TotalLoss: 0.0319\n",
      ">>> [Epoch 85] FixMatch Train Loss: 0.0328, LR: 0.000013\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "EarlyStopping counter: 13 out of 50\n",
      "[Epoch 86, Batch 10] SupLoss: 0.0012, UnsupLoss: 0.0315, TotalLoss: 0.0327\n",
      "[Epoch 86, Batch 20] SupLoss: 0.0008, UnsupLoss: 0.0266, TotalLoss: 0.0273\n",
      ">>> [Epoch 86] FixMatch Train Loss: 0.0298, LR: 0.000013\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "EarlyStopping counter: 14 out of 50\n",
      "[Epoch 87, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0347, TotalLoss: 0.0348\n",
      "[Epoch 87, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0322, TotalLoss: 0.0322\n",
      ">>> [Epoch 87] FixMatch Train Loss: 0.0323, LR: 0.000013\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 15 out of 50\n",
      "[Epoch 88, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0326, TotalLoss: 0.0329\n",
      "[Epoch 88, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0404, TotalLoss: 0.0405\n",
      ">>> [Epoch 88] FixMatch Train Loss: 0.0324, LR: 0.000013\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 16 out of 50\n",
      "[Epoch 89, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0216, TotalLoss: 0.0216\n",
      "[Epoch 89, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0278, TotalLoss: 0.0279\n",
      ">>> [Epoch 89] FixMatch Train Loss: 0.0297, LR: 0.000013\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "EarlyStopping counter: 17 out of 50\n",
      "[Epoch 90, Batch 10] SupLoss: 0.0005, UnsupLoss: 0.0247, TotalLoss: 0.0252\n",
      "[Epoch 90, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0305, TotalLoss: 0.0306\n",
      ">>> [Epoch 90] FixMatch Train Loss: 0.0286, LR: 0.000013\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8333\n",
      "EarlyStopping counter: 18 out of 50\n",
      "[Epoch 91, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0309, TotalLoss: 0.0310\n",
      "[Epoch 91, Batch 20] SupLoss: 0.0010, UnsupLoss: 0.0278, TotalLoss: 0.0288\n",
      ">>> [Epoch 91] FixMatch Train Loss: 0.0291, LR: 0.000013\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 19 out of 50\n",
      "[Epoch 92, Batch 10] SupLoss: 0.0000, UnsupLoss: 0.0230, TotalLoss: 0.0230\n",
      "[Epoch 92, Batch 20] SupLoss: 0.0003, UnsupLoss: 0.0290, TotalLoss: 0.0293\n",
      ">>> [Epoch 92] FixMatch Train Loss: 0.0306, LR: 0.000013\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 20 out of 50\n",
      "[Epoch 93, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0242, TotalLoss: 0.0244\n",
      "[Epoch 93, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0275, TotalLoss: 0.0276\n",
      ">>> [Epoch 93] FixMatch Train Loss: 0.0292, LR: 0.000013\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 21 out of 50\n",
      "[Epoch 94, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0315, TotalLoss: 0.0317\n",
      "[Epoch 94, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0297, TotalLoss: 0.0297\n",
      ">>> [Epoch 94] FixMatch Train Loss: 0.0320, LR: 0.000013\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8148\n",
      "EarlyStopping counter: 22 out of 50\n",
      "[Epoch 95, Batch 10] SupLoss: 0.0001, UnsupLoss: 0.0350, TotalLoss: 0.0351\n",
      "[Epoch 95, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0274, TotalLoss: 0.0275\n",
      ">>> [Epoch 95] FixMatch Train Loss: 0.0284, LR: 0.000006\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 23 out of 50\n",
      "[Epoch 96, Batch 10] SupLoss: 0.0004, UnsupLoss: 0.0333, TotalLoss: 0.0338\n",
      "[Epoch 96, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0211, TotalLoss: 0.0211\n",
      ">>> [Epoch 96] FixMatch Train Loss: 0.0303, LR: 0.000006\n",
      "Validation Accuracy: 92.45%, F1-Score: 0.8462\n",
      "EarlyStopping counter: 24 out of 50\n",
      "[Epoch 97, Batch 10] SupLoss: 0.0008, UnsupLoss: 0.0336, TotalLoss: 0.0344\n",
      "[Epoch 97, Batch 20] SupLoss: 0.0003, UnsupLoss: 0.0345, TotalLoss: 0.0348\n",
      ">>> [Epoch 97] FixMatch Train Loss: 0.0294, LR: 0.000006\n",
      "Validation Accuracy: 94.34%, F1-Score: 0.8800\n",
      "EarlyStopping counter: 25 out of 50\n",
      "[Epoch 98, Batch 10] SupLoss: 0.0003, UnsupLoss: 0.0253, TotalLoss: 0.0256\n",
      "[Epoch 98, Batch 20] SupLoss: 0.0002, UnsupLoss: 0.0371, TotalLoss: 0.0373\n",
      ">>> [Epoch 98] FixMatch Train Loss: 0.0293, LR: 0.000006\n",
      "Validation Accuracy: 88.68%, F1-Score: 0.7500\n",
      "EarlyStopping counter: 26 out of 50\n",
      "[Epoch 99, Batch 10] SupLoss: 0.0000, UnsupLoss: 0.0275, TotalLoss: 0.0275\n",
      "[Epoch 99, Batch 20] SupLoss: 0.0000, UnsupLoss: 0.0172, TotalLoss: 0.0173\n",
      ">>> [Epoch 99] FixMatch Train Loss: 0.0289, LR: 0.000006\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8148\n",
      "EarlyStopping counter: 27 out of 50\n",
      "[Epoch 100, Batch 10] SupLoss: 0.0002, UnsupLoss: 0.0307, TotalLoss: 0.0309\n",
      "[Epoch 100, Batch 20] SupLoss: 0.0001, UnsupLoss: 0.0283, TotalLoss: 0.0284\n",
      ">>> [Epoch 100] FixMatch Train Loss: 0.0303, LR: 0.000006\n",
      "Validation Accuracy: 90.57%, F1-Score: 0.8000\n",
      "EarlyStopping counter: 28 out of 50\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# 8. RUN THE TRAINING\n",
    "##############################################################################\n",
    "best_f1 = -np.inf\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_one_epoch_fixmatch(\n",
    "        epoch, \n",
    "        model, \n",
    "        labeled_loader, \n",
    "        unlabeled_loader, \n",
    "        focal_criterion,\n",
    "        unsup_criterion,\n",
    "        optimizer, \n",
    "        scaler, \n",
    "        accumulation_steps\n",
    "    )\n",
    "\n",
    "    val_f1 = validate_fixmatch(model, val_loader)\n",
    "\n",
    "    if epoch < warmup_epochs + 10:\n",
    "        scheduler_warmup.step()\n",
    "    else:\n",
    "        scheduler_plateau.step(val_f1)\n",
    "\n",
    "    early_stopping(val_f1, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7088e944-4575-4c52-a628-cab955bd1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33952/2513874388.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_fixmatch_model_0_90.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model from early stopping.\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_fixmatch_model_0_90.pt'))\n",
    "print(\"Loaded the best model from early stopping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0747e24b-4a12-4277-9ce5-26f12ed019f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>downloadUrl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://new-projects-team-public.s3.yandex.net...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         downloadUrl\n",
       "0  https://new-projects-team-public.s3.yandex.net...\n",
       "1  https://new-projects-team-public.s3.yandex.net...\n",
       "2  https://new-projects-team-public.s3.yandex.net...\n",
       "3  https://new-projects-team-public.s3.yandex.net...\n",
       "4  https://new-projects-team-public.s3.yandex.net...\n",
       "5  https://new-projects-team-public.s3.yandex.net...\n",
       "6  https://new-projects-team-public.s3.yandex.net...\n",
       "7  https://new-projects-team-public.s3.yandex.net...\n",
       "8  https://new-projects-team-public.s3.yandex.net...\n",
       "9  https://new-projects-team-public.s3.yandex.net..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(path):\n",
    "    data = pd.read_excel(path)\n",
    "    return data\n",
    "\n",
    "test_df = load_test_data('test_set_for_labeling.xlsx')\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92487b40-5588-496e-b847-3b3bcb9ad17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_images_from_csv(\"test_set_for_labeling.xlsx\", \"downloadUrl\", \"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76b31d25-a0e6-4f86-a3d0-52e6343c9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, data_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.data_dir, self.dataframe.iloc[idx, 0].split('/')[-1])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        # print(image.shape, image.dtype)\n",
    "        # label = self.dataframe.iloc[idx, 0]\n",
    "        # print(type(label))\n",
    "        # print(label, img_path)\n",
    "        # image = sr_model(image.to(device))[0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.dataframe.iloc[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a518eb3-00a8-4c4c-8303-7042a473783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)), #224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce3e74c3-e326-4a67-be7f-80626ebebe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 94.34%, F1-Score: 0.8571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = ImageDataset(val_df, 'labeled', transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "validate_fixmatch(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a7db9cd-cb71-4807-bd21-cb4d7dede441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.50%, F1-Score: 0.8325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8324873096446701"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = ImageDataset(labeled, 'labeled', transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=8)\n",
    "validate_fixmatch(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "146d7e0a-7cad-40ff-a63d-3244759e3583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3985b0e06f64835aff7fecad8dae812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = TreeTestDataset(test_df, data_dir=\"test_data\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True, pin_memory_device='cuda')\n",
    "predictions = []\n",
    "img_paths = []\n",
    "for inputs, paths in tqdm(test_loader):\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(torch.sigmoid(outputs), dim=-1).cpu().numpy()\n",
    "        predictions.extend(preds)\n",
    "        img_paths.extend(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9642f801-9ec4-4027-aefb-27fcc43960eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"downloadUrl,is_conifer\\n\"\n",
    "text_data = header + '\\n'.join([f\"{url},{str(bool(label)).upper()}\" for url, label in zip(img_paths, predictions)])\n",
    "# text_data\n",
    "with open(\"test_result_labels.txt\", 'w') as f:\n",
    "    f.write(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a1a50-6280-4544-a146-fe6ab318bd19",
   "metadata": {},
   "source": [
    "__Analyze__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652fa18-63d3-4574-bb9c-48fcf359ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from torchvision.io import read_image\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as out_file:\n",
    "            out_file.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "\n",
    "def download_images_from_csv(csv_file, image_column, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if csv_file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(csv_file)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_file, sep=\"\\t\")\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        url = row[image_column]\n",
    "        filename = os.path.join(output_dir, url.split('/')[-1])\n",
    "        download_image(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6135f43f-1f01-4de5-bf2c-b125342cab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdsourced_df = pd.read_csv('train_full.tsv', sep='\\t', skiprows=1,\n",
    "                              names=['downloadUrl', 'is_conifer', 'confidence'], usecols=range(3))\n",
    "crowdsourced_df = crowdsourced_df[~crowdsourced_df['downloadUrl'].isin(labeled['downloadUrl'])]\n",
    "crowdsourced_df['confidence'] = crowdsourced_df['confidence'].apply(lambda x: float(x[:-1]))\n",
    "max_conf = crowdsourced_df['confidence'].max()\n",
    "crowdsourced_df['confidence_normalized'] = crowdsourced_df['confidence'] / max_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89289bb8-af65-47a9-a714-89b74874d186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.09"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowdsourced_df['confidence'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6840e602-66c4-45ed-839d-0e79be88cebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1337, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crowdsourced_df[crowdsourced_df['confidence_normalized'] >= 0.9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2ae56f0-0be4-4082-950e-5ef488feff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crowdsourced_df[crowdsourced_df['confidence_normalized'] == 1].downloadUrl.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5578d85a-f672-4e63-bc1a-aff92f415b1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "image_names = pd.read_csv('test.csv', header=None)[0]\n",
    "\n",
    "source_folder = 'unlabeled'\n",
    "destination_folder = 'labeled'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "for image_name in image_names:\n",
    "    source_path = os.path.join(source_folder, image_name)\n",
    "    destination_path = os.path.join(destination_folder, image_name)\n",
    "\n",
    "    try:\n",
    "        # shutil.copy2(source_path, destination_path)\n",
    "        \n",
    "        image = Image.open(destination_path)\n",
    "        tensor_image = transform(image)\n",
    "\n",
    "        # plt.imshow(tensor_image.permute(1, 2, 0))\n",
    "        # plt.title(f\"Image: {image_name}\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb693e0-835e-4007-84d7-0b753b37ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_conifer\n",
       "False    886\n",
       "True     398\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_labeled_df.is_conifer.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
